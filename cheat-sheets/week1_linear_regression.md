# Week 1 â€” Linear Regression Cheat Sheet

### ğŸ§© Core Formula

The hypothesis (prediction) function:
\[
f(x) = w \cdot x + b
\]

In vector form (for multiple samples):
\[
\hat{y} = Xw + b
\]

Where:

- **x** â†’ input feature(s)
- **w** â†’ weight(s) or slope(s)
- **b** â†’ bias or intercept
- **f(x)** â†’ predicted value for input x

---

### âš™ï¸ Model Parameters â€” Î¸ (Theta)

The model parameters Î¸ represent everything the algorithm is trying to learn:
\[
Î¸ = \begin{bmatrix} b \\ w \end{bmatrix}
\]
so the model can be rewritten compactly as:
\[
f(x) = Î¸^T x
\]
_(assuming x includes a bias term 1 as its first column)_

---

### ğŸ“‰ Cost Function â€” Mean Squared Error (MSE)

The cost function measures _how far off_ the predictions are from the true values.

There are a few common ways to write it:

1ï¸âƒ£ **Expanded (scalar) form:**
\[
J(w, b) = \frac{1}{2m} \sum*{i=1}^{m} (f*{w,b}(x^{(i)}) - y^{(i)})^2
\]

2ï¸âƒ£ **Vectorized form:**
\[
J(Î¸) = \frac{1}{2m} (XÎ¸ - y)^T (XÎ¸ - y)
\]

Where:

- **m** = number of training examples
- **X** = input matrix (each row = one training example)
- **y** = true output values
- **XÎ¸ - y** = prediction errors

---

### ğŸ” Gradient Descent (Learning Algorithm)

Goal: iteratively update Î¸ to minimize J(Î¸)

**Update rule:**
\[
Î¸ := Î¸ - Î± \cdot \frac{1}{m} X^T (XÎ¸ - y)
\]

Where:

- **Î± (alpha)** = learning rate (controls step size)
- We repeat until cost stops decreasing

**Intuition:**  
Each iteration, Î¸ moves a little â€œdownhillâ€ on the cost function surface.  
Too large Î± â†’ overshoots.  
Too small Î± â†’ takes forever to converge.

---

### ğŸ’¡ Quick Insights

- **Gaussian noise:** Adds small random variation to make the dataset more realistic (mimics measurement errors).
- **Normal Equation:**  
  Direct, closed-form solution for Î¸ without iterations:
  \[
  Î¸ = (X^T X)^{-1} X^T y
  \]
  Useful for smaller datasets but not scalable for huge feature sets.
- **Gradient Descent:**  
  Works for any number of features and scales to large data; foundation for neural networks later.

---

### ğŸ§  Reflection

This week I learned how a model â€œfitsâ€ a line to data by finding Î¸ values that minimize the cost function.  
Itâ€™s the first real taste of how machines _learn from data_ â€” by adjusting parameters to reduce error.  
Both gradient descent and the normal equation reach the same solution, just in different ways.

---

### ğŸ’¬ Note

I used AI to help me structure and format this cheat sheet while keeping the explanations in my own words for clarity and quick reference.
