{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1c124e6",
   "metadata": {},
   "source": [
    "## Day 1 — Setup & Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322e5a7f",
   "metadata": {},
   "source": [
    "# Linear Regression from Scratch\n",
    "Goal: implement linear regression from scratch with NumPy.\n",
    "Steps: derive gradients, implement training loop, plot loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3563adc7",
   "metadata": {},
   "source": [
    "# Linear Regression (from scratch)\n",
    "Following Andrew Ng ML Course - Week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e7fca8",
   "metadata": {},
   "source": [
    "## Day 2 — Generate & Visualise a Synthetic Linear Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4409bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate a synthetic linear dataset\n",
    "X = 2 * np.random.rand(100, 1)     # 100 points between 0 and 2\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)  # y = 4 + 3x + Gaussian noise\n",
    "\n",
    "# Plot the data\n",
    "plt.scatter(X, y, alpha=0.7)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Synthetic Linear Dataset\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7eb7e93",
   "metadata": {},
   "source": [
    "## Day 3 — Cost Function & Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f98a8e",
   "metadata": {},
   "source": [
    "**Goal today**\n",
    "- Define hypothesis and **cost function** \\(J(\\theta)\\)\n",
    "- Implement **gradient descent**\n",
    "- Train on your synthetic data and **watch the cost decrease**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf5333c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare design matrix with bias term (1s column)\n",
    "try:\n",
    "    X  # reuse from Day 2 if present\n",
    "    y\n",
    "except NameError:\n",
    "    # Fallback: generate the same synthetic dataset if notebook was restarted\n",
    "    X = 2 * np.random.rand(100, 1)\n",
    "    y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "m = len(y)\n",
    "X_b = np.c_[np.ones((m, 1)), X]  # shape (m, 2): [1, x]\n",
    "print(\"X_b shape:\", X_b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ae32fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, theta):\n",
    "    \"\"\"Mean Squared Error cost for linear regression.\n",
    "    X: (m, n) design matrix (already includes bias column)\n",
    "    y: (m, 1) targets\n",
    "    theta: (n, 1) parameters\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    predictions = X.dot(theta)\n",
    "    errors = predictions - y\n",
    "    return (1 / (2 * m)) * np.sum(errors ** 2)\n",
    "\n",
    "# Quick sanity check\n",
    "theta_zeros = np.zeros((X_b.shape[1], 1))\n",
    "print(\"Initial cost at theta=zeros:\", compute_cost(X_b, y, theta_zeros))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00efadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta, alpha=0.1, num_iters=1000):\n",
    "    m = len(y)\n",
    "    J_hist = []\n",
    "    for _ in range(num_iters):\n",
    "        predictions = X.dot(theta)\n",
    "        errors = predictions - y\n",
    "        gradients = (1 / m) * X.T.dot(errors)\n",
    "        theta = theta - alpha * gradients\n",
    "        J_hist.append(compute_cost(X, y, theta))\n",
    "    return theta, J_hist\n",
    "\n",
    "# Train\n",
    "theta_init = np.zeros((X_b.shape[1], 1))\n",
    "theta_final, J_hist = gradient_descent(X_b, y, theta_init, alpha=0.1, num_iters=1000)\n",
    "print(\"Learned theta:\", theta_final.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c32ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cost curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(J_hist)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost J(θ)\")\n",
    "plt.title(\"Gradient Descent Convergence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa1c5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize fitted line vs data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_line = np.array([[0.0], [2.0]])\n",
    "X_line_b = np.c_[np.ones((2, 1)), x_line]\n",
    "y_line = X_line_b.dot(theta_final)\n",
    "\n",
    "plt.scatter(X, y, alpha=0.7, label=\"data\")\n",
    "plt.plot(x_line, y_line, label=\"fitted line\", linewidth=2)\n",
    "plt.xlabel(\"x\"); plt.ylabel(\"y\"); plt.title(\"Linear Regression Fit\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
