{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1c124e6",
   "metadata": {},
   "source": [
    "## Day 1 — Setup & Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c07a8ef",
   "metadata": {},
   "source": [
    "### Quick note on θ (theta)\n",
    "- **θ** just means the parameters of the line we’re learning.\n",
    "- For simple linear regression: **θ₀** is the intercept (where the line crosses y‑axis) and **θ₁** is the slope.\n",
    "- We tweak θ to make predictions line up with the data (by minimising the cost)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3563adc7",
   "metadata": {},
   "source": [
    "# Linear Regression (from scratch)\n",
    "Goal: implement linear regression from scratch with NumPy.\n",
    "Steps: derive gradients, implement training loop, plot loss.\n",
    "\n",
    "Following Andrew Ng ML Course - Week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e7fca8",
   "metadata": {},
   "source": [
    "## Day 2 — Generate & Visualise a Synthetic Linear Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4409bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)  # keep results reproducible each run\n",
    "\n",
    "# Fake (but realistic) linear data:\n",
    "# x values live roughly in [0, 2]\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "\n",
    "# The underlying rule I used to generate targets:\n",
    "# y = 4 + 3x + ε, where ε is small Gaussian (normal) noise ~ N(0, 1)\n",
    "# Noise makes points look like real measurements (never perfectly on the line).\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "# Visual check: should look like a fuzzy line with positive slope\n",
    "plt.scatter(X, y, alpha=0.7)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Synthetic Linear Dataset\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7eb7e93",
   "metadata": {},
   "source": [
    "## Days 3 to 5 — Cost Function & Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f98a8e",
   "metadata": {},
   "source": [
    "**Goal today**\n",
    "- Define hypothesis and **cost function** \\(J(\\theta)\\)\n",
    "- Implement **gradient descent**\n",
    "- Train on synthetic data and **watch the cost decrease**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf5333c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare design matrix with bias term (1s column)\n",
    "try:\n",
    "    X  # reuse from Day 2 if present\n",
    "    y\n",
    "except NameError:\n",
    "    # Fallback: generate the same synthetic dataset if notebook was restarted\n",
    "    X = 2 * np.random.rand(100, 1)\n",
    "    y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "m = len(y)\n",
    "X_b = np.c_[np.ones((m, 1)), X]  # shape (m, 2): [1, x]\n",
    "print(\"X_b shape:\", X_b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ae32fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(25.00415182181856)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculates the cost (mean squared error) between our predictions and actual values\n",
    "def compute_cost(X, y, theta):\n",
    "    \"\"\"Mean Squared Error (MSE) cost for linear regression.\n",
    "    X: (m, n) design matrix. Column 0 should be all 1s (the bias term).\n",
    "    y: (m, 1) targets\n",
    "    theta: (n, 1) parameters we’re learning (θ₀=intercept, θ₁…=slopes)\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    preds = X.dot(theta)               # model predictions h_θ(x)\n",
    "    errors = preds - y                 # residuals (how far off we are)\n",
    "    J = (1 / (2 * m)) * np.sum(errors ** 2)  # the classic MSE / 2\n",
    "    return J\n",
    "\n",
    "# Sanity check at θ = 0 (usually large, since the line is at y=0)\n",
    "try:\n",
    "    X_b\n",
    "except NameError:\n",
    "    # if ran this cell in isolation, rebuild design matrix quickly\n",
    "    import numpy as np\n",
    "    X_b = np.c_[np.ones((len(X), 1)), X]\n",
    "theta_zeros = np.zeros((X_b.shape[1], 1))\n",
    "compute_cost(X_b, y, theta_zeros)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00efadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs gradient descent to iteratively update theta and minimize the cost function\n",
    "def gradient_descent(X, y, theta, alpha=0.1, num_iters=1000):\n",
    "    \"\"\"Batch gradient descent.\n",
    "    alpha: learning rate (how big a step we take each update)\n",
    "    num_iters: number of parameter updates\n",
    "    Returns: (theta, cost_history)\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    J_hist = []\n",
    "\n",
    "    for _ in range(num_iters):\n",
    "        preds = X.dot(theta)\n",
    "        errors = preds - y\n",
    "        # gradient of MSE wrt theta: (1/m) * X^T * (preds - y)\n",
    "        grads = (1 / m) * X.T.dot(errors)\n",
    "        theta = theta - alpha * grads   # take a step downhill\n",
    "        J_hist.append(compute_cost(X, y, theta))\n",
    "\n",
    "    return theta, J_hist\n",
    "\n",
    "# Train from a simple starting point θ = 0\n",
    "theta_init = np.zeros((X_b.shape[1], 1))\n",
    "theta_final, J_hist = gradient_descent(X_b, y, theta_init, alpha=0.1, num_iters=1000)\n",
    "theta_final.ravel()  # expect ~ [intercept≈4, slope≈3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c32ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost should steadily decrease if learning rate is sensible\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(J_hist)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost J(θ)\")\n",
    "plt.title(\"Gradient Descent Convergence\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa1c5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the learned line on top of the data to eyeball the fit\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_line = np.array([[0.0], [2.0]])           # span of our X range\n",
    "X_line_b = np.c_[np.ones((2, 1)), x_line]   # add bias column\n",
    "y_line = X_line_b.dot(theta_final)          # model’s prediction\n",
    "\n",
    "plt.scatter(X, y, alpha=0.7, label=\"data\")\n",
    "plt.plot(x_line, y_line, label=\"fitted line\", linewidth=2)\n",
    "plt.xlabel(\"x\"); plt.ylabel(\"y\"); plt.title(\"Linear Regression Fit\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db781a3e",
   "metadata": {},
   "source": [
    "## Week 1 — Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa67820b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare solutions: Normal Equation vs Gradient Descent\n",
    "import numpy as np\n",
    "\n",
    "# Make sure X and y exist (regenerate the same distribution if the kernel was restarted)\n",
    "try:\n",
    "    X, y\n",
    "except NameError:\n",
    "    np.random.seed(42)\n",
    "    X = 2 * np.random.rand(100, 1)\n",
    "    y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "# Design matrix with bias term\n",
    "X_b = np.c_[np.ones((len(X), 1)), X]\n",
    "\n",
    "# 1) Normal Equation (closed-form solution)\n",
    "theta_normal = np.linalg.inv(X_b.T @ X_b) @ (X_b.T @ y)\n",
    "\n",
    "# 2) Gradient Descent (reuse if available; otherwise run a quick pass)\n",
    "try:\n",
    "    theta_final\n",
    "    theta_gd = theta_final\n",
    "except NameError:\n",
    "    def compute_cost(X, y, theta):\n",
    "        m = len(y)\n",
    "        preds = X.dot(theta)\n",
    "        errors = preds - y\n",
    "        return (1/(2*m)) * np.sum(errors**2)\n",
    "\n",
    "    def gradient_descent(X, y, theta, alpha=0.1, num_iters=1000):\n",
    "        m = len(y); J_hist = []\n",
    "        for _ in range(num_iters):\n",
    "            preds = X.dot(theta)\n",
    "            errors = preds - y\n",
    "            grads = (1/m) * X.T.dot(errors)\n",
    "            theta = theta - alpha * grads\n",
    "            J_hist.append(compute_cost(X, y, theta))\n",
    "        return theta, J_hist\n",
    "\n",
    "    theta_init = np.zeros((X_b.shape[1], 1))\n",
    "    theta_gd, _ = gradient_descent(X_b, y, theta_init, alpha=0.1, num_iters=1000)\n",
    "\n",
    "# Print out the learned parameters (θ) from both methods to compare results\n",
    "# θ (theta) represents the model parameters: θ₀ is the intercept, θ₁ is the slope\n",
    "\n",
    "print(\"θ (Normal Equation):\", theta_normal.ravel())  # Direct solution from the math formula\n",
    "print(\"θ (Gradient Descent):\", theta_gd.ravel())     # Learned by iteratively adjusting θ to reduce error\n",
    "\n",
    "# Compare how close both sets of θ values are (L2 norm = overall distance between them)\n",
    "print(\"Δθ (L2 norm):\", np.linalg.norm(theta_normal - theta_gd))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444e5764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlay predictions from both methods\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_line = np.array([[0.0], [2.0]])\n",
    "X_line_b = np.c_[np.ones((2, 1)), x_line]\n",
    "\n",
    "y_pred_normal = X_line_b @ theta_normal\n",
    "y_pred_gd = X_line_b @ theta_gd\n",
    "\n",
    "plt.scatter(X, y, alpha=0.7, label=\"data\")\n",
    "plt.plot(x_line, y_pred_normal, label=\"Normal Equation\", linewidth=2)\n",
    "plt.plot(x_line, y_pred_gd, linestyle=\"--\", label=\"Gradient Descent\")\n",
    "plt.xlabel(\"x\"); plt.ylabel(\"y\"); plt.title(\"Week 1 — Linear Regression: Solutions Agree\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b23ce6d",
   "metadata": {},
   "source": [
    "### Reflection (Week 1)\n",
    "- Both methods learn similar parameters (**θ₀ ≈ intercept**, **θ₁ ≈ slope**).  \n",
    "- **Normal Equation** gives a direct solution (fast for small features), while **Gradient Descent** scales to big feature sets and generalises to other models.  \n",
    "- Adding **Gaussian noise** made the dataset realistic — the fitted line captures the trend, not every point.  \n",
    "- Intuition locked in: we tweak **θ** to **minimise cost (MSE)** so predictions match data better over time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae5ed1f",
   "metadata": {},
   "source": [
    "### Visual Demo — How Gradient Descent Learns the Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abcacf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ab8ee87bf16469b8af676f7eceb0bd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='iter_idx', max=250), Output()), _dom_classes=('widget-in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_iteration(iter_idx=0)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Interactive visualization — slider to move through gradient descent iterations\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "# Recreate the same dataset and training if needed\n",
    "np.random.seed(42)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "X_b = np.c_[np.ones((len(X), 1)), X]\n",
    "\n",
    "# run gradient descent and store all theta values\n",
    "def gradient_descent_with_history(X, y, theta_init, alpha=0.1, num_iters=50):\n",
    "    m = len(y)\n",
    "    theta = theta_init.copy()\n",
    "    theta_hist = [theta.copy()]\n",
    "    for _ in range(num_iters):\n",
    "        preds = X.dot(theta)\n",
    "        errors = preds - y\n",
    "        grads = (1/m) * X.T.dot(errors)\n",
    "        theta -= alpha * grads\n",
    "        theta_hist.append(theta.copy())\n",
    "    return theta, theta_hist\n",
    "\n",
    "theta_init = np.zeros((X_b.shape[1], 1))\n",
    "\n",
    "# change num_iters from 30, to 100, to 250 to see how the learning process evolves\n",
    "theta_final, theta_hist = gradient_descent_with_history(X_b, y, theta_init, alpha=0.1, num_iters=30)\n",
    "\n",
    "# Normal Equation result for comparison\n",
    "theta_ne = np.linalg.inv(X_b.T @ X_b) @ (X_b.T @ y)\n",
    "\n",
    "# Setup base line range\n",
    "x_line = np.array([[0.0], [2.0]])\n",
    "X_line_b = np.c_[np.ones((2, 1)), x_line]\n",
    "\n",
    "# Define update function for slider\n",
    "def plot_iteration(iter_idx=0):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.scatter(X, y, alpha=0.7, label=\"data\")\n",
    "    plt.xlabel(\"x\"); plt.ylabel(\"y\")\n",
    "    plt.title(f\"Gradient Descent Progress — Iteration {iter_idx}\")\n",
    "\n",
    "    # normal equation (final target)\n",
    "    y_pred_ne = X_line_b @ theta_ne\n",
    "    plt.plot(x_line, y_pred_ne, \"g-\", label=\"Normal Equation\")\n",
    "\n",
    "    # current GD line\n",
    "    theta = theta_hist[iter_idx]\n",
    "    y_pred_gd = X_line_b @ theta\n",
    "    plt.plot(x_line, y_pred_gd, \"r--\", label=\"GD iteration\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# slider to move between iterations\n",
    "interact(plot_iteration, iter_idx=IntSlider(min=0, max=len(theta_hist)-1, step=1, value=0))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
